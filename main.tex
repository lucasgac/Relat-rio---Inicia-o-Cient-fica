\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6.5in, 9.5in}]{geometry}
\usepackage[portuguese]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{babel,arrows,positioning,chains,matrix,scopes,cd,quotes,calc,decorations.pathmorphing}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage[portuguese, ruled, lined]{algorithm2e}
\usepackage{setspace}
\usepackage[T1]{fontenc}
\usepackage{biblatex}
\addbibresource{bib.bib}
\usepackage{csquotes}
\usepackage{pythonhighlight}
\usepackage{algorithmic}
\usepackage[portuguese, ruled, lined]{algorithm2e}

%quiver
\tikzset{curve/.style={settings={#1},to path={(\tikztostart)
    .. controls ($(\tikztostart)!\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    and ($(\tikztostart)!1-\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    .. (\tikztotarget)\tikztonodes}},
    settings/.code={\tikzset{quiver/.cd,#1}
        \def\pv##1{\pgfkeysvalueof{/tikz/quiver/##1}}},
    quiver/.cd,pos/.initial=0.35,height/.initial=0}

% TikZ arrowhead/tail styles.
\tikzset{tail reversed/.code={\pgfsetarrowsstart{tikzcd to}}}
\tikzset{2tail/.code={\pgfsetarrowsstart{Implies[reversed]}}}
\tikzset{2tail reversed/.code={\pgfsetarrowsstart{Implies}}}
% TikZ arrow styles.
\tikzset{no body/.style={/tikz/dash pattern=on 0 off 1mm}}

\newtheorem{definition}{Definição}[section]
\newtheorem{proposition}[definition]{Proposição}
\newtheorem{lemma}[definition]{Lema}
\newtheorem{axiom}[definition]{Axioma}
\newtheorem{corollary}[definition]{Corolário}
\newtheorem{theorem}[definition]{Teorema}
\newtheorem{distribution}[definition]{Distribuição}
\newtheorem{example}[definition]{Exemplo}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\DeclareMathOperator{\freeab}{Free_{Ab}}

\DeclareMathOperator{\Top}{Top}
\DeclareMathOperator{\Ab}{Ab}
\DeclareMathOperator{\Grp}{Grp}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\module}{Mod}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\chain}{Ch}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Alt}{Alt}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\Ricci}{Ricci}
\DeclareMathOperator{\arccosh}{arccosh}
\DeclareMathOperator{\gyr}{gyr}
\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\Log}{Log}

\newcommand{\Mod}[1]{$\module_{#1}$}
\newcommand{\Chain}[1]{$\chain(#1)$}

\newcommand{\openset}[0]{{\phantom{}\subset}{\circ}\phantom{.}}

\def\arrvline{\hfil\kern\arraycolsep\vline\kern-\arraycolsep\hfilneg}

\begin{document}
\begin{titlepage}
    \center
    \textsc{\textbf{\Large Iniciação Científica}}\\[0.5cm]
    \textsc{\textbf{\large Relatório Final}}\\[0.5cm]
    
    \vspace{4cm}
    
    \rule{\linewidth}{0.5mm}\\[0.4cm]
    {\huge \textbf{Aprendizado profundo e geometria hiperbólica}}
    \rule{\linewidth}{0.5mm}\\[1.0cm]
    
    \vspace{1.0cm}
    
    \textbf{\Large{Universidade de São Paulo}}\\[0.2cm]
    \textbf{\large{Instituto de Ciências Matemáticas e Computação}}
    \vspace{4cm}
    
    \begin{flushright}
        \begin{tabular}{@{}ll@{}}
            \hspace{1cm}\textbf{\large{Aluno:}} & \textbf{\large{Lucas Giraldi Almeida Coimbra}}\\
            \hspace{1cm}\textbf{\large{Orientador:}} & \textbf{\large{Carlos Henrique Grossi Ferreira}}\\
            \end{tabular}
    \end{flushright}
    
    \vfill
    \Large{Junho de 2023}\\[0.2cm]
    \Large{São Carlos}
\end{titlepage}

\newpage

\tableofcontents

\newpage

\section{Introdução}

\section{Geometria riemanniana}

\subsection{Variedades e métricas riemannianas}

Uma \textit{variedade topológica de dimensão $n$} é um espaço topológico $M$ Hausdorff com base enumerável que é \textit{localmente euclidiano de dimensão $n$}, isso é, para cada $p \in M$ existe um aberto $U$ e um homeomorfismo $\phi \colon U \to V \openset \mathbb{R}^n$. O par $(U, \phi)$ será comumente chamado de \textit{carta sobre $p$}. Se $(V, \psi)$ é uma outra carta em $M$ tal que $U \cap V \neq \varnothing$, chamamos de \textit{mapas de transição} as funções \begin{equation}
    \phi \circ \psi^{-1} \colon \psi(U \cap V) \to \mathbb{R}^n \quad \text{e} \quad \psi \circ \phi^{-1} \colon \phi(U \cap V) \to \mathbb{R}^n.
\end{equation}
Se os mapas de transição forem suaves, diremos que $(U, \phi)$ e $(V, \psi)$ são \textit{compatíveis}. Uma \textit{estrutura diferenciável} em $M$ é uma cobertura de $M$ por cartas que são duas a duas compatíveis. Dizemos que $M$ é \textit{suave} ou \textit{diferenciável} se possuir uma estrutura diferenciável.

A partir de agora, toda carta estará em uma estrutura diferenciável previamente fixada, e portanto toda variedade será suave. Se $p \in M$ dizemos que $F \colon M \to N$ é \textit{suave em $p$} se existirem $(U, \phi)$ carta sobre $p$ e $(V, \psi)$ carta sobre $F(p)$ tais que $\psi \circ F \circ \phi^{-1}$ é suave. A função $F$ é \textit{suave em $U \openset M$} se for suave em todo ponto de $U$, e é apenas \textit{suave} se for suave em todo ponto de $M$.

Uma \textit{curva} em $M$ é um mapa suave $c \colon I \to M$ onde $I$ é um intervalo de $\mathbb{R}$. Se $p \in M$, definimos por $C^\infty_p$ como o conjunto dos mapas $f \colon U \openset M \to \mathbb{R}$ suaves, onde $U$ é uma vizinhança qualquer de $p$. Esse espaço é uma álgebra com as três operações: \begin{itemize}
    \item se $f \colon U \to \mathbb{R}$ e $g \colon V \to \mathbb{R}$, definimos $f + g \colon U \cap V \to \mathbb{R}$ por $(f+g)(p) = f(p) + g(p)$;
    \item se $f \colon U \to \mathbb{R}$ e $\lambda \in \mathbb{R}$, definimos $\lambda f \colon U \to \mathbb{R}$ por $(\lambda f)(p) = \lambda f(p)$;
    \item se $f \colon U \to \mathbb{R}$ e $g \colon V \to \mathbb{R}$, definimos $fg \colon U \cap V \to \mathbb{R}$ por $(fg)(p) = f(p)g(p)$.
\end{itemize}
Dada uma curva $c \colon ]-\varepsilon, \varepsilon[ \to M$, definimos $c'(0)$ como sendo um mapa $c'(0) \colon C_p^\infty \to \mathbb{R}$ dado por \begin{equation}
    c'(0)f = \left.\frac{d}{dt}\right|_{t = 0} (f \circ c)(t).
\end{equation}
Esse mapa é linear e satisfaz a \textit{regra de Leibniz}, isso é, \begin{equation}
    c'(0)(fg) = f(c(0)) \cdot c'(0)g + c'(0)f \cdot g(c(0)). 
\end{equation}

Se $p \in M$, o \textit{espaço tangente a $M$ em $p \in M$} como o conjunto \begin{equation}
    T_pM = \{c'(0) \mid c \colon ]-\varepsilon, \varepsilon[ \to \mathbb{R} \text{ e } c(0) = p\}.
\end{equation}
Se $M$ tem dimensão $n$, então $T_pM$ é um espaço vetorial de dimensão $n$. Seus elementos são chamados de \textit{vetores tangentes}. Uma \textit{métrica riemanniana} em $M$ é a associação de um produto interno $\mathfrak{g}_p(-,-)$ em $T_pM$ para cada $p \in M$. Mais do que isso, pedimos que essa associação seja suave. Entenderemos o que isso significa a seguir.

Um \textit{campo vetorial} em $M$ é uma associação $X$ de um vetor $X_p \in T_pM$ para cada $p \in M$. Se $\phi = (x^1, \dots, x^n)$ é uma carta sobre $p \in M$ e $r = (r^1, \dots, r^n)$ são as coordenadas em $\mathbb{R}^n$, definimos as derivadas parciais de $f \in C_p^\infty$ por \begin{equation}
    \left.\frac{\partial f}{\partial x^i}\right|_{p} = \left.\frac{\partial}{\partial r^i}\right|_{\phi(p)} (f \circ \phi^{-1})(r).
\end{equation}
Cada derivada parcial em $p$ pode ser vista como um elemento de $T_pM$, afinal, se $e^1, \dots, e^n$ é a base canônica de $\mathbb{R}^n$, então dadas as curvas $c^i(t) = t e^i$ temos \begin{equation}
    \left.\frac{\partial}{\partial x^i}\right|_p = (\phi^{-1} \circ c^i)'(0).
\end{equation}
Esses vetores tangentes formam uma base para $T_pM$.

Se $(U, \phi)$ é uma em $M$ e $X$ é um campo vetorial em $M$, então para cada $p \in M$ podemos escrever, de maneira única, \begin{equation}
    X_p = \sum_{k = 1}^n a^i(p)\left.\frac{\partial}{\partial x^i}\right|_p.
\end{equation}
Dizemos que o campo vetorial $X$ é \textit{suave} se existir uma cobertura de $M$ por cartas tais que os mapas $a^i$ são sempre suaves. Ao dizermos que a métrica riemanniana tem que ser suave, queremos dizer que, para quaisquer $X, Y$ campos suaves em $M$, o mapa $p \mapsto \mathfrak{g}_p(X_p, Y_p)$ tem que ser suave. Uma \textit{variedade riemanniana} é uma variedade suave equipada com uma métrica riemanniana.

\subsection{Conexões e derivada covariante}

Denotamos o conjunto de todos os campos suaves em $M$ por $\mathfrak{X}(M)$. Se $M = \mathbb{R}^n$, vamos entender quem é a derivada direcional. Se $X = (v^1, \dots, v^n) \in \mathbb{R}^n$ e $X_p$ é o vetor tangente a $p$ na direção $X$, então dada $f \colon \mathbb{R}^n \to \mathbb{R}$ definimos a \textit{derivada direcional de $f$ na direção $X_p$} \begin{equation}
    D_{X_p}f = \lim_{t \to 0} \frac{f(p + tX) - f(p)}{t} = \sum_{k = 1}^n v^k \left.\frac{\partial f}{\partial x^i}\right|_p =  X_pf.
\end{equation}
Podemos então trocar $f$ por um campo vetorial suave $Y = \sum b^i \partial/\partial x^i$ e obtermos a \textit{derivada direcional de $Y$ na direção $X_p$} \begin{equation}
    D_{X_p}Y = \sum_{k = 1}^n D_{X_p}b^i \left.\frac{\partial}{\partial x^i}\right|_p.
\end{equation}
Note que a derivada $D_{X_p}Y$ é um vetor tangente em $p$. Dessa forma, se $X$ é um campo vetorial em $\mathbb{R}^n$ podemos definir $D_XY$ como o campo vetorial que, em $p$, vale $D_{X_p}Y$. Esse mapa é a \textit{derivada direcional de $Y$ na direção $X$}.

Agora vamos generalizar a derivada direcional em $\mathbb{R}^n$ para uma variedade riemanniana qualquer. Uma \textit{conexão afim} em $M$ é um mapa \begin{align*}
    \nabla \colon \mathfrak{X}(M) \times \mathfrak{X}(M) &\to \mathfrak{X}(M) \\ (X, Y) &\mapsto \nabla_XY
\end{align*}
que satisfaz as seguintes propriedades:
\begin{itemize}
    \item se $C^\infty(M)$ é o conjunto dos mapas suaves $M \to \mathbb{R}$, então $\nabla$ é $C^{\infty}(M)$-linear na primeira coordenada;
    \item $\nabla$ satisfaz a regra de Leibniz na segunda coordenada, isso é, se $f \in C^{\infty}(M)$, então \begin{equation}
        \nabla_X(fY) = (Xf)Y + f\nabla_XY,
    \end{equation} onde $Xf$ é o mapa suave dado por $(Xf)(p) = X_pf$.
\end{itemize}

Conexões e métricas riemannianas não estão sempre conectadas. Porém, se $M$ é uma variedade riemanniana e $\nabla$ uma conexão afim em $M$, então podemos falar sobre alguns aspectos geométricos de $\nabla$. Definimos o \textit{tensor torção} de $\nabla$ como sendo o mapa $T(X, Y) = \nabla_XY - \nabla_YX - [X,Y]$, onde $[X,Y]_pf = X_p(Yf) - Y_p(Xf)$ é o \textit{bracket de Lie}. Do mesmo modo, definimos o \textit{tensor curvatura} de $\nabla$ como sendo o mapa $R(X,Y) = [\nabla_X, \nabla_Y] - \nabla_{[X,Y]}$, isso é, para um campo vetorial suave $Z$, temos \begin{equation}
    R(X,Y)Z = \nabla_X\nabla_YZ - \nabla_Y\nabla_XZ - \nabla_{[X, Y]}Z.
\end{equation}

Dizemos que uma conexão $\nabla$ em uma variedade riemanniana $M$ é \textit{compatível com a métrica} se $Z\mathfrak{g}(X,Y) = \mathfrak{g}(\nabla_ZX, Y) + \mathfrak{g}(X, \nabla_ZY)$. Uma \textit{conexão de Levi-Civita} é uma conexão compatível com a métrica e que satisfaz $T(X,Y) = 0$ para todos $X, Y$ campos suaves em $M$.

\begin{proposition}
    Toda variedade riemanniana possui uma, e apenas uma, conexão de Levi-Civita.
\end{proposition}

Um campo vetorial \textit{ao longo} de uma curva $c \colon I \to M$ é a associação $V$ de um vetor $V(t) \in T_{c(t)}M$ para cada $t \in I$. Dizemos que $V$ é \textit{suave} se, para cada $f \colon M \to \mathbb{R}$, $(Vf)(t) = V(t)f$ é suave.

Se $c \colon I \to \mathbb{R}^n$ é uma curva e $V$ é um campo ao longo de $c$, temos \begin{equation}
    V(t) = \sum_{k = 1}^n v^i(t)\left.\frac{\partial}{\partial x^i}\right|_{c(t)},
\end{equation}
portanto podemos definir a \textit{derivada de $V$ com respeito a $t$} como sendo o campo \begin{equation}
    \frac{dV}{dt} = \sum_{k = 1}^n \frac{dv^i}{dt}\frac{\partial}{\partial x^i}.
\end{equation} Essa derivada satisfaz algumas propriedades importantes:
\begin{itemize}
    \item ela é linear com respeito a $V$, isso é, se $\lambda \in \mathbb{R}$ e $U$ é outro campo ao longo de $c$, então \begin{equation}
        \frac{d(\lambda V + U)}{dt} = \lambda\frac{dV}{dt} + \frac{dU}{dt};
    \end{equation}

    \item ela satisfaz a regra de Leibniz, isso é, se $f \colon I \to \mathbb{R}$ (lembrando aqui que $I$ é o domínio de $c$) é suave, então \begin{equation}
        \frac{d(fV)}{dt} = \frac{df}{dt}V + f\frac{dV}{dt};
    \end{equation}

    \item ela é compatível com a derivada direcional em $\mathbb{R}^n$, isso é, se $V$ se estende para um campo $\tilde{V}$ em $\mathbb{R}^n$, então \begin{equation}
        \frac{dV}{dt} = D_{c'(t)}\tilde{V}.
    \end{equation}
\end{itemize}

Vamos agora generalizar o conceito da derivada de $V$ para uma variedade $M$ qualquer, utilizando de conexões afins. Se $\nabla$ é uma conexão afim em $M$ e $c \colon I \to \mathbb{R}$ é uma curva, então definimos uma \textit{derivada covariante} como um operador $D/dt$ que, para cada campo $V$ ao londo de $c$ associa um outro campo $DV/dt$ ao longo de $c$. Pedimos que essa associação satisfaça as três propriedades que a derivada definida acima satisfaz: \begin{itemize}
    \item $D/dt$ é linear, isso é, se $V$ e $U$ são campos ao longo de $c$ e $\lambda \in \mathbb{R}$ então \begin{equation}
        \frac{D(\lambda V + U)}{dt}  = \lambda\frac{DV}{dt} + \frac{DU}{dt};
    \end{equation}
    
    \item $D/dt$ satisfaz a regra de Leibniz, isso é, se $f \colon I \to \mathbb{R}$ é suave, então \begin{equation}
        \frac{D(fV)}{dt} = \frac{df}{dt}V + f\frac{DV}{dt};
    \end{equation}

    \item $D/dt$ é compatível com a conexão afim, isso é: se $\tilde{V}$ é um campo em $M$ que estende $V$, então \begin{equation}
        \frac{DV}{dt} = \nabla_{c'(t)}V.
    \end{equation}
\end{itemize}

Definimos acima o que seria \textbf{uma} derivada covariante, mas acontece que, fixadas uma conexão e uma curva, sempre existe uma e apenas uma derivada covariante, portanto podemos falar \textbf{da} derivada covariante.

\subsection{Jacobiana e teorema da função inversa}

Se $F \colon M \to N$ é suave, então para todas as carta $(U, \phi)$ sobre $p$ e $(V, \psi)$ sobre $F(p)$ o mapa $\psi \circ F \circ \phi^{-1}$ é suave. Sabemos da teoria de variedades que as derivadas parciais $\partial/\partial \phi^i$ e $\partial/\partial \psi^j$ formam base para $T_pM$ e $T_{F(p)}N$, respectivamente. Considere agora a transformação linear $D_pF$ dada por \begin{equation}
    D_pF(v) f = v(f \circ F)
\end{equation} que manda vetores tangentes a $p$ para vetores tangentes a $F(p)$. Na expressão acima, estamos apenas descrevendo como o vetor $D_pF(v)$ age em uma função $f \colon N \to \mathbb{R}$ suave. O mapa $D_pF$ é chamado de \textit{derivada de $F$ em $p$}.

Podemos então considerar a matriz de $D_pF$ conforme as bases $\partial/\partial \phi^i$ e $\partial/\partial \psi^j$. Se denotarmos por $F^i$ o mapa $\psi^i \circ F$, então temos que \begin{equation}
    D_pF = \begin{bmatrix}
        \left.\frac{\partial F^i}{\partial \phi^j}\right|_p
    \end{bmatrix}.
\end{equation} Note que ela coincide com a matriz jacobiana que conhecemos do cálculo. De fato, essa coincidência motiva uma nova versão do teorema da função inversa.

\begin{theorem}
    Se $F \colon M \to N$ é suave e $\dim M = \dim N$, então $F$ é um difeomorfismo local em $p \in M$ se, e somente se, $\det D_pF \neq 0$.
\end{theorem}

\subsection{Geodésicas e transporte paralelo}

Se $c \colon I \to M$ é uma curva, então dizemos que $c$ é uma \textit{geodésica} se a derivada covariante $DT/dt$ do seu campo velocidade $T(t) = c'(t)$ é nula. Note que a existência de uma conexão, e portanto de uma derivada covariante, não depende da existência de uma métrica riemanniana. Porém, caso a variedade $M$ possua uma métrica, vamos sempre assumir que a conexão considerada é a conexão de Levi-Civita em $M$.

\begin{proposition}
    Geodésicas em variedades riemannianas possuem velocidade constante, isso é, se $c \colon I \to M$ é uma geodésica, então $||c'(t)||$ é constante para cada $t \in I$.
\end{proposition}

Seja $M$ uma variedade suave com uma conexão $\nabla$. Se $(U, x^1, \dots, x^n)$ é uma carta em $M$, então temos os campos vetoriais $\partial_i = \partial/\partial x^i$. Sabemos que todo campo vetorial em $U$ se escreve como combinação linear destes, e portanto temos \begin{equation}
    \nabla_{\partial_i}\partial_j = \sum_{k = 1}^n \Gamma^k_{ij} \partial_k.
\end{equation}
Os coeficientes $\Gamma^k_{ij}$ são chamados de \textit{símbolos de Christoffel de $\nabla$ em $(U, x^1, \dots, x^n)$}.

Sejam $M$ uma variedade com uma conexão $\nabla$, $(U, \phi) = (U, x^1, \dots, x^n)$ uma carta em $M$ e $\Gamma^k_{ij}$ os seus símbolos de Christoffel. Note que, se $c \colon I \to M$ é uma curva e $y = \phi \circ c$, então temos \begin{equation}
    T = c'(t) = \sum_{k = 1}^n \frac{dy^k}{dt}\partial_k.
\end{equation} Dessa maneira, segue que \begin{align}
    \frac{DT}{dt} &= \sum_{j = 1}^n \frac{d^2y^j}{dt^2}\partial_j + \sum_{j = 1}^n \frac{dy^j}{dt}\frac{D\partial_j}{dt} = \sum_{j = 1}^n \frac{d^2y^j}{dt^2}\partial_j + \sum_{j = 1}^n \frac{dy^j}{dt} \nabla_{c'(t)}\partial_j \\ &= \sum_{j = 1}^n \frac{d^2y^j}{dt^2}\partial_j + \sum_{i,j = 1}^n \frac{dy^j}{dt} \nabla_{\frac{dy^i}{dt}\partial_i}\partial_j = \sum_{j = 1}^n \frac{d^2y^j}{dt^2}\partial_j + \sum_{i,j = 1}^n \frac{dy^j}{dt}\frac{dy^i}{dt}\nabla_{\partial_i}\partial_j \\ &= \sum_{k = 1}^n \frac{d^2y^k}{dt^2}\partial_k + \sum_{i,j,k = 1}^n \frac{dy^j}{dt}\frac{dy^i}{dt}\Gamma^k_{ij}\partial_k = \sum_{k = 1}^n \left(\frac{d^2y^k}{dt^2} + \sum_{i,j=1}^n \frac{dy^i}{dt}\frac{dy^j}{dt}\Gamma_{ij}^k\right)\partial_k.
\end{align} Portanto, temos o seguinte resultado.

\begin{theorem}
    Se $M$ é uma variedade suave com uma conexão $\nabla$ e $c \colon I \to M$ é uma curva, então $c$ é uma geodésica se, com respeito a qualquer carta $(U, \phi) = (U, x^1, \dots, x^n)$, as componentes de $y = \phi \circ c$ satisfazem o sistema de EDOs \begin{equation}
        \frac{d^2y^k}{dt^2} + \sum_{i,j=1}^n \frac{dy^i}{dt}\frac{dy^j}{dt}\Gamma_{ij}^k = 0
    \end{equation}
\end{theorem}

As equações do sistema acima são chamadas de \textit{equações geodésicas}. Pelo teorema de existência e unicidade de solução para EDOs temos a existência e unicidade de geodésicas.

\begin{theorem}\label{geodesicas_existencia}
    Seja $M$ uma variedade suave com uma conexão $\nabla$. Dado $p \in M$ e $X_p \in T_pM$, existe uma geodésica $c \colon I \to M$ tal que $c(0) = p$ e $c'(0) = X_p$. Mais do que isso, essa geodésica é única no sentido de que qualquer outra geodésica satisfazendo essas propriedades deve coincidir com $c$ na intersecção de seus domínios.
\end{theorem}

Um \textit{difeomorfismo} entre variedades suaves $M$ e $N$ é um mapa $F \colon M \to N$ suave, bijetor e com inversa suave. Se $M$ e $N$ forem riemannianas, dizemos que $F$ é uma \textit{isometria} se, para todos $p \in M$ e $X_p, Y_p \in T_pM$, temos \begin{equation}
    \mathfrak{g}_p(X_p, Y_p) = \mathfrak{g}_{F(p)}(D_pF(X_p), D_pF(Y_p)).
\end{equation}

\begin{proposition}
    Isometrias preservam conexões de Levi-Civita. Mais ainda, mapas que preservam conexões, preservam geodésicas. Como corolário, isometrias preservam geodésicas.
\end{proposition}

Se $c \colon I \to M$ é uma curva e $V$ é um campo ao longo de $c$, então dizemos que $V$ é \textit{paralelo} se $DV/Dt = 0$. Dessa forma, uma geodésica é uma curva cujo campo velocidade é paralelo. Fixado $X_p \in T_{c(t_0)}M$, existe um único campo $V$ ao longo de $c$, paralelo, tal que $V(t_0) = X_p$. Se $c \colon [a,b] \to M$ é uma curva e $V$ é um campo paralelo ao longo de $c$, dizemos que $V(b)$ é obtido a partir de $V(a)$ por \textit{translação paralela}. Dizemos que $V(b)$ é o \textit{transporte paralelo} de $V(a)$ ao longo de $c$.

\begin{proposition}
    Se $V$ e $W$ são paralelos ao longo de $c$ em uma variedade riemanniana $M$, então $||V||$ e $\mathfrak{g}(V, W)$ são constantes.
\end{proposition}

Um problema importante com questão ao transporte paralelo é a existência. Ela está garantida pelo resultado abaixo.

\begin{theorem}
    Se $M$ é uma variedade suave com uma conexão $\nabla$ e $c \colon [a,b] \to M$ uma curva. Dado $v \in T_{c(a)}M$, existe um campo vetorial paralelo $V_t$ ao longo de $c$ tal que $V_a = v$.
\end{theorem}

\subsection{Mapa exponencial e mapa logarítmico}

Uma geodésica $c \colon I \to M$ é \textit{maximal} se não podemos estender $c$ para um intervalo maior do que $I$ sem que a curva deixe de ser uma geodésica. Do Teorema \ref{geodesicas_existencia} temos que, dado $p \in M$ e $X_p \in T_pM$ existe uma única geodésica maximal $c$ com $c(0) = p$ e $c'(0) = X_p$. Vamos denotar essa geodésica por $\gamma_{X_p}$.

O \textit{mapa exponencial} em um ponto $p \in M$ é a função dada por $\Exp_p(X_p) = \gamma_{X_p}(1)$. Esse mapa não está necessariamente definido para todo $X_p \in T_pM$, visto que nem sempre $\gamma_{X_p}$ possui $1$ no seu domínio. Uma variedade com uma conexão é dita \textit{completa} se toda geodésica puder ter seu domínio estendido para todo $\mathbb{R}$. No caso de variedades riemannianas consideradas com a conexão de Levi-Civita, temos dois resultados que nos ajudam no sentido de definir $\Exp_p$ para um conjunto satisfatório de vetores.

\begin{proposition}
    Para qualquer $p \in M$, com $M$ variedade riemanniana, existem uma vizinhança $U$ de $p$ e dois números $\epsilon, \delta > 0$ tais que para todos $q \in U$ e $v \in T_qM$ com $||v|| < \delta$, existe uma única geodésica $\gamma \colon ]-\varepsilon, \varepsilon[ \to M$ com $\gamma(0) = q$ e $\gamma'(0) = v$.
\end{proposition}

\begin{corollary} \label{geodesicas_intervalo}
    Para qualquer $p \in M$, com $M$ variedade riemanniana, existem uma vizinhança $U$ de $p$ e um número $\delta > 0$ tais que para todos $q \in U$ e $v \in T_qM$ com $||v|| < \delta$ existe uma única geodésica $\gamma \colon ]-2, 2[ \to M$ com $\gamma(0) = q$ e $\gamma'(0) = v$.
\end{corollary}

O Corolário \ref{geodesicas_intervalo} nos diz que o mapa exponencial está sempre definido em todas as direções, porém essa existência só está garantida para velocidades pequenas. Se você for muito rápido, pode ficar cansado muito rápido e não dar tempo do seu conjunto de parâmetros englobar o $1$.

\begin{proposition}
    A derivada $D_0\Exp_p$ é a identidade em $T_pN$ para qualquer $p \in M$.
\end{proposition}

A Proposição acima garante, em particular, que sempre existe um $\varepsilon > 0$ tal que $\Exp_p$ mapeia $B(0, \varepsilon)$ difeomorficamente em $M$. Por causa disso, existe uma inversa para o mapa exponencial, que chamaremos de \textit{mapa logarítmico}.

\subsection{Curvatura por via de tensores}

Fixada uma conexão $\nabla$ em $M$, já conhecemos o tensor de torção, que é dado por \begin{align}
    T(X,Y) = \nabla_X Y - \nabla_Y X - [X,Y].
\end{align} Ao tomarmos $X = \partial_i$ e $Y = \partial_j$ em uma carta $(U, \phi)$, temos $[X,Y] = \partial_i \partial_j - \partial_j \partial_i = 0$, portanto se a conexão $\nabla$ é a de Levi-Citiva, temos pelo anulamento da torção que $\nabla_{\partial_i} \partial_j = \nabla_{\partial_j} \partial_i$. Dessa maneira, as derivadas desses campos comutam.

Vamos tentar entender o que acontece para o tensor de curvatura. Comecemos lembrando que ele é dado por \begin{align}
    R(X,Y)Z = \nabla_X\nabla_Y Z - \nabla_Y\nabla_X Z - \nabla_{[X,Y]}Z.
\end{align} Ao tomarmos $X = \partial_i$, $Y = \partial_j$ e $Z$ qualquer, temos novamente que $[X,Y] = 0$, portanto $\nabla_{[X,Y]}Z = 0$. Dessa forma, segue que \begin{align}
    R(\partial_i, \partial_j)Z = \nabla_{\partial_i}\nabla_{\partial_j}Z - \nabla_{\partial_j}\nabla_{\partial_i}Z.
\end{align} Porém, mesmo que $\nabla$ seja a conexão de Levi-Civita, não temos garantia de que $R(X,Y)Z = 0$, dessa forma nem sempre derivar um campo em direções diferentes independe da ordem dessas derivadas. O que vai medir a diferença entre essas operações é a curvatura da sua variedade.

Para entendermos a curvatura geometricamente, precisamos falar de holonomia. Dado $p \in M$ e $\gamma \colon [a,b] \to M$ fechada em $p$ e contrátil, para cada $v \in T_pM$ podemos considerar o campo $V_t$ ao longo de $\gamma$ que seja paralelo e satisfaça $V_a = v$. O vetor $v' = V_b$, que é o transporte paralelo de $v$ ao longo de $\gamma$, é chamado de \textit{holonomia de $v$ ao longo de $\gamma$}.

A menos que sua variedade possua curvatura $0$, isso é, se $R(X,Y)Z = 0$ para todos $X, Y, Z$, a sempre existirá $v \in T_pM$ tal que $v' \neq v$. Ou seja, o ângulo entre esses dois vetores é também medido pela curvatura.

Agora vamos entender quem são as possíveis curvaturas de uma variedade riemanniana. Por enquanto, vamos dar enfoque em três principais tipos: a seccional (ou gaussiana), a de Ricci e a escalar.

A primeira coisa a notar é que, dados $x, y, z \in T_pM$, podemos construir campos suaves $X, Y, Z$ ao redor de $p$ de maneira que $X_p = x$, $Y_p = y$ e $Z_p = z$. Definimos então $R(x,y)z$ como sendo o campo $R(X, Y)Z$ no ponto $p$.

\begin{theorem}
    Seja $p \in M$ com $\dim M \geq 2$ e $W \leq T_pM$ um subespaço de dimensão $2$ (também conhecido como plano). Considere $x, y$ uma base para $W$ e defina o número \begin{equation}
        K(W) = \frac{\mathfrak{g}_p(R(x,y)x, y)}{|x \wedge y|^2},
    \end{equation} onde $|x \wedge y|$ é a área do paralelogramo formado por $x$ e $y$, que pode ser explicitamente calculada por \begin{equation}
        |x \wedge y| = \sqrt{||x||^2||y||^2 - \mathfrak{g}_p(x,y)},
    \end{equation} onde $||\cdot||$ é a norma induzida pela métrica $\mathfrak{g}$. Temos então que $K(W)$ não depende da base escolhida para $W$.
\end{theorem}

A quantidade $K(W)$ definida no teorema acima é a \textit{curvatura seccional} ou \textit{curvatura gaussiana} de $W$ em $p$. Agora podemos usar essa curvatura para falarmos de curvatura de Ricci. Dado $p \in M$ e $v \in T_pM$ unitário, considere $w \in v^\perp$. Se $P_w = \mathbb{R}v + \mathbb{R}w$, então temos a curvatura seccional $K(P_w)$. A \textit{curvatura de Ricci em $p$ na direção $v$} é a média de todas essas curvaturas seccionais, ou seja, \begin{equation}
    \Ricci_p(v) = \lambda\int_S K(P_w)dV,
\end{equation} onde $S$ é a esfera unitária em $T_pM$, $dV$ é a forma de volume em $S$ e $\lambda$ é uma constante positiva que é, honestamente, irrelevante. De fato, ela pode ser calculada explicitamente se utilizarmos a seguinte proposição.

\begin{proposition}
    Se $w_1, \dots, w_{n-1}$ é uma base ortonormal de $v^\perp$, então \begin{equation}
        \Ricci_p(v) = \frac{1}{n-1}\sum_{i = 1}^{n - 1} \mathfrak{g}_p(R(x, z_i)x, z_i).
    \end{equation}
\end{proposition}

Probabilisticamente, podemos pensar que curvatura de Ricci é a curvatura seccional média de planos aleatórios da forma $P_w$. Isso nos dá alguma ideia do porquê não conseguimos recuperar as curvaturas seccionais a partir da curvatura de Ricci: há perda de informação, pois a partir da média de um conjunto de dados raramente conseguimos recuperar quem são esses dados.

Por fim, a \textit{curvatura escalar} nada mais é do que uma média das curvaturas de Ricci, ou seja, se $p \in M$, podemos considerar uma base ortonormal $z_1, \dots, z_n$ de $T_pM$. A curvatura escalar é o número definido por \begin{equation}
    K_s(p) = \frac{1}{n}\sum_{i = 1}^n \Ricci_p(z_i).
\end{equation}

Por mais que a curvatura escalar não dependa de uma direção, e apenas do ponto, não é a ela que nos referimos ao dizer que $M$ tem curvatura constante $\kappa$ em $p \in M$. Essa expressão diz que todas as curvaturas seccionais em $p$ valem $\kappa$.
    
\subsection{Conceitos métricos}

Antes de brincarmos com a geometria hiperbólica, vamos falar de duas definições que aparecem na teoria de espaços métricos e que podem ser úteis mais para frente. Dado um espaço métrico $(X, d)$, o \textit{produto de Gromov} é uma operação que, dados três pontos $x, y, z \in X$, retorna o número \begin{equation}
    (y, z)_x = \frac{1}{2}(d(x,y) + d(x,z) - d(y,z)).
\end{equation} Dizemos que $X$ é \textit{$\delta$-hiperbólico}, com $\delta > 0$, se para todos $x, y, z, w \in X$ temos \begin{equation}
    (x,z)_w \geq \min\{(x,y)_w, (y,z)_w\} - \delta.
\end{equation}

Uma definição equivalente envolve triângulos geodésicos. Uma \textit{geodésica} em $(X, d)$ é a imagem isométrica de um intervalo $[a,b]$. Se $\gamma$ é essa isometria, $x = \gamma(a)$ e $y = \gamma(b)$, denotamos a imagem de $\gamma$ por $[x, y]$. Um \textit{triângulo geodésico com vértices $x, y, z \in X$} é a união das geodésicas $[x,y]$, $[y, z]$ e $[z, x]$. Se para cada $m \in [x,y]$ existe um ponto em $n \in [y,z] \cup [z,w]$ tal que $d(m,n) < \delta$, dizemos que o triângulo geodésico $\Delta(x,y,z)$ é \textit{$\delta$-fino}. Dizemos que $(X, d)$ é \textit{$\delta$-hiperbólico} se todo triângulo geodésico é $\delta$-fino.

Por fim, precisamos falar de distorção, que é uma medida de fidelidade para certos mergulhos de dados em aprendizado de máquina. Se $X$ e $Y$ são espaços métricos e $f \colon X \to Y$ é um mergulho de dados em $X$ para pontos de $Y$, a \textit{distorção de $f$ em $x, y \in X$} é dada por \begin{equation}
    \mathbb{D}_f(a,b) = \frac{|d(a,b) - d(f(a), f(b))|}{d(a,b)}.
\end{equation}

\section{Modelos para a geometria hiperbólica}

O espaço hiperbólico real é uma variedade riemanniana de curvatura constante igual a $-1$. Existem diversos modelos isométricos para ele, e agora vamos falar de alguns. Aqui, é importante notar que todos esses modelos estão mergulhados 

\subsection{O semi-espaço de Poincaré}

O primeiro modelo do qual vamos falar é o semi-espaço de Poincaré. Ele é dado, como variedade suave, pelo semi-espaço \begin{align}
    \mathbb{H}^n = \{x \in \mathbb{R}^n \mid x_n > 0\}.
\end{align} Sua métrica, por sua vez, é dada pela expressão \begin{align}
    ds^2 = \frac{dx_1^2 + \cdots + dx_n^2}{x_n^2}.
\end{align} Essa expressão significa que, para cada $p \in \mathbb{H}^n$, temos \begin{align}
    \mathfrak{g}_p(u, v) = \frac{u_1v_1 + \cdots u_nv_n}{p_n^2}.
\end{align} As formas diferenciais $dx_i$ recebem os vetores $u, v$ e retornam as respectivas coordenadas. Por sua vez, sempre que $x_i$ aparecer em uma fórmula, ele será substituído pela $i$-ésima coordenada do ponto onde a métrica está sendo construída.

Esse espaço, assim como toda variedade riemanniana, possui uma estrutura de espaço métrico induzida pela métrica riemanniana. Essa métrica, em $\mathbb{H}^n$, é dada por \begin{align}
    d(x,y) = \arccosh\left(1 + \frac{||x - y||^2}{2x_ny_n}\right).
\end{align}

\subsection{O hiperboloide de Lorentz}

O modelo do hiperboloide depende do que chamamos de \textit{métrica de Lorentz} em $\mathbb{R}^{n+1}$. Ela é definida por \begin{align}
    \langle x, y \rangle_\mathbb{L} = -x_0y_0 + x_1y_1 + \cdots + x_ny_n.
\end{align} Como variedade suave, o hiperboloide é definido por \begin{align}
    \mathbb{L}^n = \{x \in \mathbb{R}^{n+1} \mid \langle x, x \rangle_\mathbb{L} = -1, x_0 > 0\}.
\end{align}

A métrica riemanniana em $\mathbb{L}^n$ é induzida também da métrica de Lorentz, e é dada por \begin{align}
    ds^2 = -dx_0^2 + dx_1^2 + \cdots + dx_n^2.
\end{align} Note que essa métrica não faz muito sentido, visto que o espaço tangente de $\mathbb{L}^n$ deveria ter dimensão $n$, mas aqui utilizamos $n+1$ coordenadas. De fato, essa métrica se aplica apenas ao considerarmos o espaço tangente $T_p\mathbb{H}^n$ como o conjunto $p^\perp = \{x \in \mathbb{R}^n \mid \langle x, p \rangle_\mathbb{L} = 0\}$. A distância no hiperboloide, por sua vez, é dada por \begin{align}
    d(x,y) = \arccosh(-\langle x, y \rangle_\mathbb{L}).
\end{align}

É importante notarmos aqui que a métrica riemanniana que definimos não parece ser um produto interno, justamente pelo fator negativo $-dx_0^2$. De fato, a métrica de Lorentz não é um produto interno em $\mathbb{R}^n$, mas ao restringirmos ela ao espaço $p^\perp$ para qualquer $p \in \mathbb{L}^n$, essa restrição é sempre um produto interno.

\subsection{O \texorpdfstring{$n$}{n}-disco de Poincaré}

O $n$-disco de Poincaré é, como variedade suave, dado por \begin{align}
    \mathbb{B}^n = \{x \in \mathbb{R}^n \mid ||x|| < 1\}.
\end{align} Sua métrica é definida como \begin{align}
    ds^2 = 4\frac{dx_1^2 + \cdots + dx_n^2}{(1 - x_1^2 + \cdots + x_n^2)^2}
\end{align} e sua distância por \begin{align}
    d(x,y) = \arccosh\left(1 + 2\frac{||x - y||^2}{(1 - ||x||^2)(1 - ||y||^2)}\right).
\end{align}

\subsection{O \texorpdfstring{$n$}{n}-disco de Beltrami-Klein}

O $n$-disco de Beltrami-Klein é, como variedade suave, dado por \begin{align}
    \mathbb{K}^n = \{x \in \mathbb{R}^n \mid ||x|| < 1\}.
\end{align} É a mesma variedade suave que da origem ao disco de Poincaré, mas a denotamos por uma letra diferente apenas para diferenciar os dois modelos. A diferença é na métrica riemanniana, que em $\mathbb{K}^n$ é dada por \begin{align}
    ds^2 = \frac{dx_1^2 + \cdots dx_n^2}{1 - x_1^2 - \cdots - x_n^2} + \frac{(x_1dx_1 + \cdots + x_ndx_n)^2}{(1 - x_1^2 - \cdots - x_n^2)^2}.
\end{align} Como consequência, a distância nesse espaço é diferente da distância no disco de Poincaré, e nesse caso é dada por \begin{align}
    d(x,y) = \frac{1}{2}\ln\left(\frac{||a - x|| \cdot ||b - y||}{||a - y|| \cdot ||b - x||}\right).
\end{align} Aqui, $a$ e $b$ são pontos construídos a partir de $x$ e $y$ pelo seguinte método: considere a reta $r$ que passa por $x$ e $y$ e defina por $a$ e $b$ os pontos em que $r$ intersecta $\mathbb{S}^{n-1}$. O ponto $a$ será escolhido como o que estiver mais próximo a $x$, e o ponto $b$ por consequência será escolhido como o que estiver mais próximo a $y$.

\subsection{O modelo do hemisfério}

O último modelo que iremos visitar é o do hemisfério. Como variedade suave, ele é dado por \begin{align}
    \mathbb{J}^n = \{x \in \mathbb{S}^n \mid x_{n+1} > 0\},
\end{align} e possui métrica dada por \begin{align}
    ds^2 = \frac{dx_1^2 + \cdots + dx_{n+1}^2}{x_{n+1}^2}.
\end{align} A distância em $\mathbb{J}^n$, por sua vez, é dada por \begin{align}
    d(x,y) = \arccosh\left(\langle \phi(x), \phi(y) \rangle_\mathbb{L}\right),
\end{align} onde $\phi$ é o mapa dado por \begin{align}
    (x_1, \dots, x_{n+1}) \mapsto \left(\frac{x_1}{x_{n+1}}, \dots, \frac{x_n}{x_{n+1}}, \frac{1}{x_{n+1}}\right).
\end{align}

\subsection{Isometrias entre os modelos}

Devemos agora explicitar isometrias entre os modelos definidos acima. Para isso, definiremos apenas quatro desses mapas e, como composição de isometrias é uma isometria, definir apenas essas quatro funções nos dará isometrias entre quaisquer dois modelos por meio de tomar inversas e compor.

\begin{itemize}
    \item O isomorfismo entre $\mathbb{L}^n$ e $\mathbb{B}^n$ é dado por \begin{align}
        (x_1, \dots, x_{n+1}) \in \mathbb{L}^n \mapsto \left(\frac{x_2}{1 + x_1}, \dots, \frac{x_{n+1}}{1 + x_1}\right) \in \mathbb{B}^n;
    \end{align}

    \item O isomorfismo entre $\mathbb{B}^n$ e $\mathbb{H}^n$ é dado por \begin{align}
        (x_1, \dots, x_n) \in \mathbb{B}^n \mapsto \frac{1}{1 + 2x_1 + ||x||^2}(1 - ||x||^2, 2x_2, \dots, 2x_n) \in \mathbb{H}^n;
    \end{align}

    \item O isomorfismo entre $\mathbb{L}^n$ e $\mathbb{K}^n$ é dado por \begin{align}
        (x_1, \dots, x_{n+1}) \in \mathbb{L}^n \mapsto \left(\frac{x_2}{x_1}, \dots, \frac{x_{n+1}}{x_1}\right) \in \mathbb{K}^n;
    \end{align}

    \item O isomorfismo entre $\mathbb{L}^n$ e $\mathbb{J}^n$ é dado por \begin{align}
        (x_1, \dots, x_{n+1}) \in \mathbb{L}^n \mapsto \left(\frac{x_1}{x_{n+1}}, \dots, \frac{x_n}{x_{n+1}}, \frac{1}{x_{n+1}}\right) \in \mathbb{J}^n.
    \end{align}
\end{itemize}

\subsection{Generalizando operações euclidianas}

O próximo passo agora é generalizar algumas operações de espaços euclidianos para os espaços hiperbólicos. Faremos isso pois, para construir redes neurais em espaços euclidianos, precisamos de álgebra linear, e portanto é uma boa ideia aprender como fazer álgebra linear em um espaço que não é vetorial. Um \textit{girogrupo} é um conjunto $G$ munido de uma operação binária $\oplus$ satisfazendo as seguintes propriedades: \begin{itemize}
    \item existe ao menos um elemento $0 \in G$ tal que $0 \oplus a = a$ para todo $a \in G$. Todo elemento satisfazendo essa condição é chamado de \textit{identidade à esquerda}.
    \item existe alguma identidade à esquerda $0 \in G$ de maneira que, para todo $a \in G$, existe $\ominus a \in G$ de maneira que $\ominus a \oplus a = 0$;
    \item para todos $a, b, c \in G$, existe um elemento $\gyr[a,b]c \in G$ tal que vale a igualdade $a \oplus (b \oplus c) = (a \oplus b) \oplus \gyr[a,b]c$;
    \item O mapa $\gyr[a,b] \colon c \mapsto \gyr[a,b]c$ é um automorfismo de $G$, isso é, é uma bijeção que satisfaz $\gyr[a,b](c \oplus d) = \gyr[a,b]c \oplus \gyr[a,b]d$;
    \item vale a \textit{propriedade de redução à direita}, isso é, $\gyr[a,b] = \gyr[a \oplus b, b]$ para todos $a, b \in G$.
\end{itemize} Um girogrupo $G$ é \textit{girocomutativo} se para todos $a, b \in G$ vale $a \oplus b = \gyr[a,b](b \oplus a)$.

O principal exemplo de girogrupo para o estudo de aprendizado profundo é o \textit{girogrupo de Möbius}. Considere o $n$-disco de Poincaré $\mathbb{B}^n$ e defina nele a \textit{soma de Möbius} dada por \begin{equation}
    x \oplus y = \frac{(1 + 2\langle x, y \rangle + ||y||^2)x + (1 - ||x||^2)y}{1 + 2\langle x, y \rangle + ||x||^2 ||y||^2}.
\end{equation} O par $(\mathbb{B}^n, \oplus)$ é um girogrupo girocomutativo, que é chamado de girogrupo de Möbius de raio $1$. Em um contexto mais geral, poderíamos tomar o interior de qualquer esfera centrada em $0$ em qualquer espaço vetorial real com produto interno, mas para nossos estudos isso não será necessário.

No girogrupo de Möbius podemos definir algumas operações extremamente importantes para a construção de redes neurais: \begin{itemize}
    \item o \textit{produto por escalar de Möbius} é definido por \begin{equation}
        \lambda \otimes x = \begin{cases}
            \tanh(\lambda \arctanh ||x||)\frac{x}{||x||}, &\text{se } x \neq 0, \\ 0, &\text{se } x = 0;
        \end{cases}
    \end{equation}

    \item podemos aplicar uma matriz $M \in M_n(\mathbb{R})$ em $x \in \mathbb{B}^n$ pela operação \begin{equation}
        M^\otimes(x) = \tanh\left(\frac{||Mx||}{||x||}\arctanh ||x||\right)\frac{Mx}{||Mx||};
    \end{equation}
\end{itemize}

Utilizando essas operações e algum conhecimento sobre geodésicas em $\mathbb{B}^n$ podemos derivar expressões explicitas para os mapas exponencial e logarítmico: \begin{equation}
    \Exp_p(v) = p \oplus \left(\tanh\left(\frac{\lambda_p||v||}{2}\right)\frac{v}{||v||}\right) \quad \text{e} \quad \Log_p(q) = \frac{2}{\lambda_p}\arctanh(||-x \oplus y||)\frac{-x \oplus y}{||-x \oplus y||}.
\end{equation} onde $p, q \in \mathbb{B}^n$, $v \in T_p\mathbb{B}^n$ e $\lambda_x$ é um fator de conformalidade entre a métrica euclidiana e a métrica do disco de Poincaré, isso é, $\lambda_p$ é dado por \begin{equation}
    \lambda_p = \frac{2}{1 - ||p||^2}
\end{equation} e claramente satisfaz \begin{equation}
    4\frac{dx_1^2 + \cdots + dx_n^2}{(1 - x_1^2 - \dots - x_n^2)^2} = \lambda_x^2(dx_1^2 + \cdots + dx_n^2),
\end{equation} ou seja, se $\mathfrak{g}$ é a métrica em $\mathbb{R}^n$ e $\mathfrak{g}_B$ é a métrica no $n$-disco, então $\mathfrak{g}_B = \lambda_x^2 \mathfrak{g}$.     

Se estamos estudando um conjunto de dados em $\mathbb{B}^n$, é interessante sabermos computar a média desses dados. Existem três maneiras de fazer isso: \begin{itemize}
    \item se os dados fazem parte de um grafo, então podemos computar a média de todos os vizinhos $x_j$ de um ponto $x_i \in \mathbb{B}^n$ pela fórmula \begin{equation}
        \mu = \Exp_{x_i}\left(\sum_{j \in \mathcal{N}(i)} w_{ij} \Log_{x_i}(x_j)\right),
    \end{equation} onde cada $w_{ij} \in \mathbb{R}$ é um peso associado a aresta que liga $x_i$ com $x_j$;

    \item se $x_1, \dots, x_n \in \mathbb{K}^n$ (agora estamos no disco e Beltrami-Klein), podemos computar o \textit{ponto médio de Einstein} por \begin{equation}
        \mu = \frac{\sum_{i = 1}^n \frac{x_i}{||x_i||^2}}{\sum_{i = 1}^n \frac{1}{||x_i||^2}};
    \end{equation}

    \item por último, podemos utilizar as operações já definidas para construir o ponto médio entre $x_1, \dots, x_n \in \mathbb{B}^n$, que é chamado de \textit{ponto giromédio} dado por \begin{equation}
        m(x_1, \dots, x_n, \alpha) = \frac{1}{2} \otimes \left(\sum_{i = 1}^n \frac{\frac{2\alpha_i}{||x_i||^2}}{\sum_{j = 1}^n \alpha_j\left(\frac{2}{||x_i||^2} - 1\right)}x_i\right),
    \end{equation} onde $\alpha = (\alpha_1, \dots, \alpha_n)$ é uma lista de pesos para cada $x_i$.
\end{itemize}

\section{Redes neurais}

\subsection{\texorpdfstring{\pyth{PyTorch}}{PyTorch} e tensores}

Para nossos estudos, definiremos um tensor de maneira indutiva. Um tensor de dimensão $0$ é um número. Um tensor de dimensão $1$ é uma lista de números, e seu formato é o número de elementos, denotado por $(n)$. Um tensor de dimensão $2$ é uma lista de tensores de dimensão $1$, que devem ter todos o mesmo formato, ou seja, o mesmo número de elementos. O formato de um tensor bidimensional é um par ordenado $(m,n)$, onde $m$ é o número de tensores de dimensão $1$ que o compõe, e $n$ o número de elementos de cada um desses tensores unidimensionais.

De maneira indutiva, um tensor de dimensão $n$ é uma lista de tensores de dimensão $n-1$, que devem todos ter o mesmo formato. O formato de um tensor $n$-dimensional é uma lista de $n$ números $(x_1, \dots, x_n)$, onde $x_1$ é o número de tensores $(n-1)$-dimensionais que o compõe, e $(x_2, \dots, x_n)$ é o formato de cada um deles.

Vamos agora começar a brincar com programação. Utilizaremos \pyth{Python} com a biblioteca \pyth{PyTorch} para mexer com aprendizado profundo. Para importar essa biblioteca, utilizamos \pyth{import torch}. O próximo passo é entender como funcionam os tensores. Para criar um tensor, utilizamos o comando \pyth{torch.tensor()}. Dentro dos parênteses, podemos colocar $4$ parâmetros (existem mais deles, mas esses são os importantes): \begin{itemize}
    \item o primeiro parâmetro é um tensor, ou seja, uma lista de listas de listas e etc. assim como definimos nos parágrafos acima;
    \item \pyth{dtype} recebe um tipo de dado, que o tensor irá armazenar em cada uma de suas entradas;
    \item \pyth{device} recebe o dispositivo no qual o tensor irá ser armazenado, como uma \textit{CPU}, uma \textit{GPU} ou uma \textit{TPU};
    \item \pyth{requires_grad} recebe \pyth{True} ou \pyth{False} e, em caso de \pyth{True}, computa o gradiente do tensor (veremos o que é isso mais para frente) e o armazena na memória.
\end{itemize}

Podemos retornar o formato, o tipo dos dados ou a dimensão de um tensor pelos atributos \pyth{shape}, \pyth{dtype} e \pyth{ndim}. Por exemplo, considere o tensor: \begin{python}
import torch # A biblioteca foi importada pois eh a primeira vez que escrevemos codigo em
# bloco, mas isso sera evitado nas proximas instancias.

TENSOR = torch.tensor([[1, 2],
                       [3, 4]])
\end{python}
Podemos utilizar os atributos comentados acima para retornar certas informações importantes: \begin{itemize}
    \item \pyth{TENSOR.shape} vai retornar \pyth{torch.Size([2, 2])}, visto que \pyth{TENSOR} tem formato $(2,2)$;
    \item \pyth{TENSOR.dtype} vai retornar \pyth{torch.int64}, visto que as entradas da matriz são números inteiros;
    \item \pyth{TENSOR.ndim} vai retornar \pyth{2}, visto que o tensor em questão tem dimensão $2$, que é a quantidade de números do seu formato.
\end{itemize}

Podemos indexar tensores. Para retirar um número de um tensor, precisamos especificar as coordenadas desse número em cada dimensão do tensor. Por exemplo, considere o código abaixo. \begin{python}
TENSOR = torch.tensor([[[1, 2, 3],
                        [4, 5, 6],
                        [7, 8, 9]],
                        
                       [[10, 11, 12],
                        [13, 14, 15],
                        [16, 17, 18]],
                        
                       [[19, 20, 21],
                        [22, 23, 24],
                        [25, 26, 27]]])

print(TENSOR.shape)
\end{python}

O output desse código será, como vimos acima, \pyth{torch.Size([3, 3, 3])}. É importante notar que o \pyth{Python} sempre começa a contar do $0$, e não é diferente quanto trabalhamos com \pyth{PyTorch}.

Para descobrirmos a posição em que o número $15$ está, basta fazermos o processo de trás pra frente. Primeiro, percebemos que ele está na segunda matriz, então ele faz parte de \pyth{TENSOR[1]}. Nessa matriz, ele está segunda linha, portanto ele é um elemento de \pyth{TENSOR[1,1]}. Por fim, ele é o terceiro elemento dessa linha, dessa maneira temos \pyth{TENSOR[1,1,2] = 15}. De fato, se após o código acima escrevermos \pyth{print(TENSOR[1,1,2])} o resultado será precisamente \pyth{tensor(15)}. O retorno não é precisamente \pyth{15} pois elementos de tensores também são tensores, mas isso é apenas uma tecnicalidade.

Existem diversas maneiras de construir tensores em \pyth{Python}. Além de \pyth{torch.tensor}, podemos utilizar \begin{itemize}
    \item \pyth{torch.ones}, que recebe diversos argumentos, um para cada dimensão que você deseja que o tensor tenha. Esses argumentos precisam ser números maiores do que $1$ e representam quantas entradas o tensor terá nas respectivas dimensões. Por exemplo, \pyth{torch.ones(2,2,2)} resulta no tensor abaixo;
    \begin{python}
tensor([[[1., 1.],
         [1., 1.]],

        [[1., 1.],
         [1., 1.]]])
    \end{python}

    \item \pyth{torch.zeros} é idêntico ao \pyth{torch.ones}, mas o tensor terá $0$ em todas as entradas;
    
    \item \pyth{torch.rand} recebe argumentos da mesma maneira que os comandos acima, mas as entradas do tensor serão números aleatórios no intervalo $[0,1[$ gerados a partir de uma distribuição uniforme;
    
    \item \pyth{torch.zeros_like} e \pyth{torch.ones_like} ambos recebem um tensor como entrada e retornam um outro tensor cheio de zeros, no primeiro caso, ou uns, no segundo, que tenha o mesmo formato do tensor recebido;
    
    \item \pyth{torch.full_like} recebe um tensor e um valor numérico, e cria um tensor com o mesmo formato do tensor recebido, mas em que todas as entradas são idênticas ao valor recebido;

    \item \pyth{torch.arange} recebe três parâmetros numéricos \pyth{start}, \pyth{end} e \pyth{step}. Ele retorna um tensor unidimensional que possui valores em $[start, end[$, começando em \pyth{start} e com um espaçamento de tamanho \pyth{step} entre cada valor. 
\end{itemize}

Tensores só aceita valores numéricos, mas mesmo dentre esses existem vários tipos que podem ser utilizados, e muitas vezes esses tipos geram conflitos quando vamos operar com tensores. É importante portanto aprender a converter tensores de um tipo para outro.

É importante notar que conversão entre tipos não vai, na maioria das vezes, alterar os valores armazenados no tensor. A diferença, por exemplo, é que o tensor \pyth{[1]} possui um valor do tipo inteiro, enquanto o tensor \pyth{[1.]} possui um valor do tipo ponto flutuante. O problema acontece quando convertemos pontos flutuantes para inteiros, visto que acontece um processo de arredondamento.

\begin{python}
TENSOR = torch.tensor([1.44]) # TENSOR.dtype = torch.float32
print(TENSOR.type(torch.int64))
\end{python}

O código acima converte um tensor do tipo \pyth{torch.float32} para um tensor do tipo \pyth{torch.int64} e exibe o resultado. Ao executarmos esse código em uma IDE, percebemos que o retorno é \pyth{tensor([1])}, visto que o tensor resultante deve possuir apenas inteiros. É importante que, ao contrário do que pensamos que seria o correto, o arredondamento é sempre feito para baixo, e não seguindo a regra do $5$ como nos acostumamos. É possível achar uma lista com todos os possíveis tipos de valores na documentação do \pyth{PyTorch}.

Vamos agora aprender a operar com tensores. Existem cinco operações importantes: \begin{itemize}
    \item a adição de tensores se dá elemento a elemento, ou seja, se \pyth{T} e \pyth{S} são $k$-tensores (tensores com $k$ dimensões), então dado \pyth{K = T + S} temos \pyth{K[x1, ..., xk] = T[x1, ..., xk] + S[x1, ..., xk]}. Por exemplo, \begin{python}
T = torch.tensor([[1, 2],
                  [3, 4]])
S = torch.tensor([[5, 6]
                  [7, 8]])
K = T + S
print(K[0,1])
    \end{python} retorna \pyth{tensor(8)}, já que \pyth{T[0,1] + S[0,1] = 2 + 6 = 8}.

    \item a subtração, a multiplicação e a divisão funcionam exatamente da mesma maneira, e são calculadas por \pyth{T - S}, \pyth{T * S} e \pyth{T / S} respectivamente. É importante notar que, para a divisão, o tensor \pyth{S} não pode possuir valores valendo \pyth{0};
\end{itemize}

A última operação que vamos analisar é o produto matricial, mas esse é assustadoramente mais complicado então vamos ter que fazer tudo com calma. Ele é definido a partir e vários casos, da seguinte maneira: sejam \pyth{T} e \pyth{S} dois tensores. Se \begin{itemize}
    \item \pyth{T} e \pyth{S} tiverem formato $(n)$, então \pyth{K = torch.matmul(T, S)} tem formato $()$ e é dado por \begin{equation}
        K = \sum_{j = 0}^{n - 1} T[j] \cdot S[j];
    \end{equation}

    \item \pyth{T} tem formato $(n)$ e \pyth{S} tem formato $(n, m)$, então \pyth{K = torch.matmul(T, S)} tem formato $(m)$ e é dado por \begin{equation}
        K[i] = \sum_{j = 0}^{n-1} T[j] \cdot S[j, i];
    \end{equation}

    \item \pyth{T} tem formato $(n, m)$ e \pyth{S} tem formato $(m)$, então \pyth{K = torch.matmul(T, S)} tem formato $(n)$ e é dado por \begin{equation}
        K[i] = \sum_{j = 0}^{m-1} T[i,j] \cdot S[j];
    \end{equation}

    \item \pyth{T} tem formato $(n,m)$ e \pyth{S} tem formato $(m,p)$, então \pyth{K = torch.matmul(T, S)} tem formato $(n,p)$ e é dado por \begin{equation}
        K[i,j] = \sum_{k = 0}^{m-1} T[i, k] \cdot S[k, j];
    \end{equation}

    \item \pyth{T} tem formato $(x_{k-1})$ e \pyth{S} tem formato $(x_1, \dots, x_k)$ com $k > 2$, então \pyth{K = torch.matmul(T, S)} tem formato $(x_1, \dots, x_{k-2}, x_k)$ e é dado por \begin{equation}
        K[i_1, \dots, i_{k-1}] = \sum_{j = 0}^{x_{k-1} - 1} T[j] \cdot S[i_1, \dots, i_{k-2}, j, i_{k-1}]
    \end{equation}

    \item \pyth{T} tem formato $(x_1, \dots, x_k)$ com $k > 2$ e \pyth{S} tem formato $(x_{k})$, então \pyth{K = torch.matmul(T, S)} tem formato $(x_1, \dots, x_{k-1})$ e é dado por \begin{equation}
        K[i_1, \dots, i_{k-1}] = \sum_{j = 0}^{x_k - 1} T[i_1, \dots, i_{k-1}, j] \cdot S[j];
    \end{equation}

    \item \pyth{T} tem formato $(n, x_{k-1})$ e \pyth{S} tem formato $(x_1, \dots, x_k)$ com $k > 2$, então \pyth{K = torch.matmul(T, S)} tem formato $(x_1, \dots, x_{k-2}, n, x_k)$ e é dado por \begin{equation}
        K[i_1, \dots, i_k] = \sum_{j = 0}^{x_{k-1} - 1} T[i_{k-1},j] \cdot S[i_1, \dots, i_{k-2}, j, i_k];
    \end{equation}

    \item \pyth{T} tem formato $(x_1, \dots, x_k)$ com $k > 2$ e \pyth{S} tem formato $(x_k, m)$, então \pyth{K = torch.matmul(T, S)} tem formato $(x_1, \dots, x_{k-1}, m)$ e é dado por \begin{equation}
        K[i_1, \dots, i_k] = \sum_{j = 0}^{x_k - 1} T[i_1, \dots, i_{k-1}, j] \cdot S[j, i_k];
    \end{equation}
\end{itemize}

Os últimos dois casos são os mais complicados. Primeiro, definimos que dois formatos $(x_1, \dots, x_k)$ e $(y_1, \dots, y_s)$ são \textit{transmissíveis} se $k, s \geq 1$, ou seja, se ambos não são escalares, e se, para todo $i$ com $1 \leq i \leq \max\{k, s\}$ temos que \begin{itemize}
    \item $y_i$ ou $x_i$ não existem, ou
    \item $y_i = 1$ ou $x_i = 1$ ou,
    \item $x_i = y_i$.
\end{itemize}
Se $k > 2$, \pyth{T} tem formato $(x_1, \dots, x_k)$ e \pyth{S} tem formato $(y_1, \dots, y_k)$, então se $(x_1, \dots, x_{k-2})$ e $(y_1, \dots, y_{k-2})$ são transmissíveis e $x_k = y_{k-1}$, podemos calcular \pyth{K = torch.matmul(T, S)}, que tem formato \begin{align}
    (\max\{x_1, y_1\}, \dots, \max\{x_{k-2}, y_{k-2}\}, x_{k-1}, y_k)
\end{align} e é dado por \begin{equation}
    K[i_1, \dots, i_k] = \sum_{j = 0}^{x_k - 1} T[\alpha_1, \dots, \alpha_{k-2}, i_{k-1}, j] \cdot S[\beta_1, \dots, \beta_{k-2}, j, i_k],
\end{equation} onde $\alpha_n = \min(i_n, x_n - 1)$ e $\beta_m = \min(i_m, y_m - 1)$.

O último caso é o que acontece se os tensores não tem a mesma dimensão. Se \pyth{T} tem formato $(x_1, \dots, x_k)$ e \pyth{S} tem formato $(y_1, \dots, y_s)$ com $k \neq s$ e $k, s > 2$, então temos dois casos: $k > s$ ou $k < s$. Se $k > s$, vamos construir um novo tensor \pyth{S1} com dimensão $k$ e formato dado por $(1, \dots, 1, y_1, \dots, y_s)$ onde as $k-s$ primeiras entradas valem $1$. Esse tensor, claro, é dado por \begin{equation}
    S_1[0, ..., 0, i_1, \dots, i_s] = S[i_1, \dots, i_s].
\end{equation} Ao assumirmos que \pyth{T} e \pyth{S} são transmissíveis (ou seja, os seus formatos menos as duas últimas dimensões são transmissíveis), então \pyth{T} e \pyth{S1} também serão, e portanto pelo caso anterior podemos calcular \pyth{K = torch.matmul(T,S1)}, que será precisamente o tensor resultante ao calcularmos \pyth{K = torch.matmul(T, S)}. O caso $k < s$ é análogo, porém o tensor que será modificado é o \pyth{T}.

Para terminarmos a seção, podemos construir um análogo à transposição de matrizes, mas para tensores. Se \pyth{T} é um tensor de formato $(x_1, \dots, x_k)$, o seu \textit{transposto} é de formato $(x_k, \dots, x_1)$ e é dado por \begin{equation}
    T^\perp[i_1, \dots, i_k] = T[i_k, \dots, i_1].
\end{equation} Para calcular o transposto pelo \pyth{PyTorch}, basta utilizar \begin{python}
torch.permute(T,list(torch.arange(T.ndim - 1, -1, -1))).
\end{python} A função \pyth{torch.permute} recebe um tensor de dimensão $k$ e uma lista com os números de $0$ até $k-1$ em qualquer ordem. Ela retorna então um novo tensor $k$-dimensional, contendo as mesmas entradas do tensor original, mas com o formato trocado de maneira a respeitar a lista de números. Para ser um pouco mais construtivo, imagine que \pyth{T} tem formato $(x_1, \dots, x_k)$. Se \pyth{l = [n1, ..., nk]} é uma lista com os inteiros de $0$ até $k-1$, então o tensor \pyth{T1 = torch.permute(T, l)} é dado por \begin{equation}
    T_1[i_1, \dots, i_k] = T[i_{n_1 + 1}, \dots, i_{n_k + 1}].
\end{equation} Note que, para transpor um tensor $k$-dimensional, basta que a lista passada como argumento de \pyth{torch.permute} seja \pyth{[k-1, ..., 0]}, que é justamente o resultado de \begin{python}
list(torch.arange(T.ndim - 1, -1, -1))
\end{python}

\subsection{Trabalhando com tensores}



\nocite{*}
\printbibliography
\end{document}